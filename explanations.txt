the first command that we always need to do is to specify the base image or a known image
(image that docker has access to - is on docker hub) the base image is the official node image.
because really thats the only thing that i need to run , and anything else is needed to run node
i got my specific node version -> with node -v command

FROM node:18.14.1

this command sets our working directory of our container, he knows that /app directory exists 
in the container because he have run the node image before and he knows it got a /app directory
setting the working directory is helpful because anytime you run a command its going to run from this directory
so i can put all of our app code in /app and run node index.js file on /app

WORKDIR /app

the next thing is to copy my package.json file( which contains all the dependencies for my app ) 
into my docker image
i wrote package.json because this is the path to it because its in my root directory.
the thing after the package.json is the directory to copy it into in my image.
 the . means the current directory ( which is /app as i set before)

COPY package.json .

once i have my package.json i need to install all of the dependencies, so i need to run the command npm install
in dockerfile to run a command - use run.
RUN npm install
now all the dependencies are  installed
now i am going to copy the rest of my files and source code into my image

COPY . ./

expose the port my app listen to (port 3000)

EXPOSE 3000

when we start the container - tell it which command to run
since my app is a node app and the entry point is index.js file
so the command it need to run is node index.js

CMD [ "node","index.js" ]

so when i deploy my container its going to run index.js
 the run command - is at build time(building the image) ,
 and the cmd command is at runtime(when run the container)

to build the image now , in the terminal and stop my express server from running on my local machine,
because i am no longer going to develop on my local machine , i am going to develop in the docker container
so to build the image is the command - docker build <the path to my dockerfile which is in my corrent directory so i'll write . >
in short this is my command:

docker build .

when i run this command you can see in the output, where its looks like for example - [1/5] is doing the first command out of the 5 
in the dockerfile, which is taking the image node:18 out of docker hub . the seconed step - [2/5] - is setting the working directory 
to be /app , after the first time i'll do it - i'll see cached written brcause its caches the result of every step.
with the command:

docker image ls 

- i can see the created image. he saw that we didnt named the image - 
so he deleted the image - with the command: 

docker image rm < image id >

and now well do the build image command with a flag to name the image:

docker build -t node-app-image .

to run the image / specify the image we want to create a container from:

docker run -d --name node-app node-app-image

--name node-app -> flag for naming the container.
don't be confused , node-app-image - is the name of the image and node-app is the name for the container.
-d flag for running the container in detached mode ( to not be attached to the cli or the console - so my command line is still free)

docker ps 
 
- to see the containers data

now i'll see that when i go back to my site its not running , this is because we are unable to connect to out docker container
on localhost 3000, even though we wrote expose 3000 -> this live does nothing , this is more for documentation -> 
for telling that this image expacts you to open up port 3000 to work.
so this line doesnt open the port 3000, docker containers by default can talk to the outside world but not the opposite.
to make the outside world can talk to our docker container(which means not just the internet but also our locallhost machine - my windows machine)
we need to poke a hole at my host machine -> telling to our host machine that if we receive traffic on a specific port we want to forward that traffic to our docker container.
to kill the container:

docker rm node-app -f  

-> -f flage means force kill, because usually you need to stop a container before killing it.
the correct command is ->

docker run -p 3000:3000 -d --name node-app node-app-image


the right 3000 is the port that were going to send traffic to our container(which is the port that our container expects to get traffic on- which what i set in the express listen app port)
the left 3000 is the port my local machine gets traffic from the outside world

when i log in to my docker container - and take a look at my files in there - with the command:

docker exec -it node-app bash

-it , for interactive mode.
bash - is a command to take a look at file system of our container
by default its going to drop us in  the slash app directory 
when i insert ls command -> i will see all the files in that  directory - and will see that dockerfile and nodemodules are in this container - which we don't need 
the dockerfile is there to create an image but we don't need it in our docker container  also node modules is very large and we don't need it were doing npm install already , ( that install all the files in the package.json file)
its because the command COPY . ./ -> that copies all the files in our current directory into the image.
also there gonna be more files we don't want like .env etc.. so we need to create a dockerignore file .

now the nodemodules that in the container is from the command npm install and not from the copy command.

when ill change the code - and save when ill refresh the page , it wont be updated - this because we need to rebuild the - 
we build an image and made a container out of it , now that we change our code - but the original image didn't changed -> and so our container didnt changed
so to update it - we need to delete the container ( by force) and then build again the image. and run the container out of the image.
-> we don't want to do that every time we change the code -> we are going to use volumes. -> which allows us to have a persistence data in our containers
theres a specific volume called a bind mount in docker  -> allows us to sync a folder(or a file system) 
in our local host machine to a folder in our docker container. that takes all the files and sync them into the /app directory 
of our container so i don't need to always rebuild the image when there is changes.
for that will delete the container, and theres no need to rebuild the image , the image is fine.

we going to update the run command of the container : 
docker run -v <copy the path to the folder with the source code on my local machine>:<the folder in our docker container> -p 3000:3000 -d --name node-app node-app-image 

to make it shorter were going to use variables - its going to be different based of my operating system.
in windows cmd :
%cd% - current directory - that way i don't need to copy the entire path ( remember you can't use the . )
so the full command now is :

docker run -v %cd%:/app -p 3000:3000 -d --name node-app node-app-image 
we still need to do another step - remember in express or node app every time we change the code - we need to restart the node process.
we will install nodemon as a dev dependency. just on the local machine (npm install nodemon --save-dev)
 because we dont need it in production ,
 and also add this 2 scripts in package.json:
   "start": "node index.js",
   "dev": "nodemon index.js",

if theres any problems in windows with nodemon not actually restarting ( i may need to pass the -L flag)
so :   "dev": "nodemon -L index.js",
and also in dockerfile now we need to change the command for running the container from 
CMD [ "node","index.js" ] -> CMD [ "npm","run","dev"]

so now that we have changed the package.json file -> i need to rebuild the image now

important !!! when ill delete the local nodemodules -> the container crashes ! even thoug we are not running our code in our locall developement 
so why is that ? its because the bind volume sync this change ( the deleting of nodemodules with out container directory )
for not doing that will use another volume 


when i delete the nodemodules in my local environment ( local machine) -> its not suppose to effect of my 
code , because its running in the docker environment. but because i used the bind voloume when i delete the nodemodules
it also delete the nodemodules in the /app in the container , so what i need to de - is make sure this bind mount doesnt
override the /app/nodemodules.
so delete the container , and add an anonymous volume to the the run command(all volumes based of specificity)

docker run -v ${PWD}:/app -v /app/node_modules -p 5000:5000 -d --name node-app node-app-image

one thing to point out:
we do a copy of all of our files into our container at the build stage ( in dockerfile )
the bind mount is just in our developement process( because thats when were changing the code)
when we deploy in production its not gonna be a bind mount(were not going to change the code in production)
what it meand the run command is not going to be with the voloume !

attention - the /app directory in the container is sync with our source code directory on our local machine
and now its a 2 way street , meaning if i make changes in our /app directory in the container its going to 
affect our source code , and that probably not what we want , so we can change our bind mount to be 
a read only bind mount.
so at the end of the bind mount we add :ro -> for read only , so the command looks like that :

docker run -v ${PWD}:/app:ro -v /app/node_modules -p 5000:5000 -d --name node-app node-app-image 

// environment variables in docker container
lets say we want our express server to listen on port 4000, remember that our express app listen on port 4000 now
we need to change the port that were sending traffic to ( because now were sending traffic to our container at port 3000 
- the right port number) so ill need to chage the right port also 
to sum up the command looks like this now:

docker run -v ${PWD}:/app:ro -v /app/node_modules --env PORT=4000 -p 5000:4000 -d --name node-app node-app-image

but this command is fine if you have small amount of environment variables

if i have many -> make an .env file and do this command instead:

docker run -v ${PWD}:/app:ro -v /app/node_modules --env-file ./.env -p 5000:4000 -d --name node-app node-app-image

when i see in the command:

docker volume ls

this is because in the running command - in this part (-v /app/node_modules) i create an anonymous volume ,
so every time i delete the container its going to preserve this volume ( this node_modules folder in this case)

docker volume prune

to delete all the volumes that isn't associated with the running container
the command for creating a container is very long!!! it becoming a problem when i run multiple containers 
there is a way to automate it - feature called docker compose.
where we can create a file that has all the steps and all the configuration settings we want for each 
docker container.

so first ill create a docker compose file,

1.  Create a File: create a file named docker-compose.yml, it uses YAML syntax.

2.  At the top of the file, define the version of the Docker Compose file format you're using.
    As of my last update, version '3' was common, 
    but always check the Docker Compose documentation for the latest version and features.

3.  Define Services: Each container you want to run is defined as a service.

---- spacing matters in YAML files !!! put always one tab
first the name of the service(ours in node-app , basically can be everything i want)
after is the build step - which image we use , we build our dockerfile in our current directory
so we pass the path to our dockerfile.
by the way when i use a docker compose file we can use relative paths so i don't need the variables

services:
  node-app:
    build:
      context: ./path-to-node-app
      dockerfile: Dockerfile
    ports:
      - "3000:3000"

Notes:

The build directive tells Docker Compose to build the Docker image from a Dockerfile located in the specified directory.
The ports directive maps ports from the host to the container.
The image directive tells Docker Compose to use a pre-built image from Docker Hub (or another registry).
The volumes directive for MongoDB is mapping a named volume to persist data across container restarts.

then do the command docker-compose up -d

technically when i run docker-compose it creates a brand new network for all my services
so it doesnt interfere with any of the other docker containers

one thing to notice:

i am running the command npm run dev (CMD [ "npm","run","dev"])
so how do i go into production?
because right now when i run docker compose everything is with respect to our development environment
my docker compose creates that bind mount - ./:/app - which we don't want in production deployment
( we dont need to sync anything) and also in production theres going to be
different environment variables and ports.

the easiest solution is you can create multiple docker files(one for development and one for production)
and on top of it we can create different docker-compose files

in the example he shows how to do everything as much in one file as possible
one dockerfile and split the docker-compose file to 2 different files.

so created instead 3 files - docker-compose.yml, docker-compose.prod.yml , docker-compose.dev.yml
the docker-compose.yml going to have all the configurations that are shared between both the 
environments( dev and prod). in an actual project were going to have ton of containers and will see that 
a lot of the configurations for our containers are the same. 

this command to run this:
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d 
-f is the flag for file, the order matters , because the fields in the second file 
override the first file , -d for detached mode.

if we run our docker-compose.prod.yml file we still download the nodemon package even though its clearly 
sets in the package.json as  "devDependencies" we are never gonna use nodemon in production,
and thats because we run in the dockerfile npm install .
normally in production its looks like that 
RUN npm install --only=production // thats prevents any dev dependencies from getting installed
so what we will do is set our dockerfile to be intelligent enough to know  whether we are in dev or
 prod mode. ( basically we write an embedded bash script)

ARG NODE_ENV
RUN if [ "$NODE_ENV" = "development" ]; \
      then npm install; \
      else npm install --only=production; \
      fi

also $NODE_ENV this is an argument that gets pass into our dockerfile when its building the docker image
 and we have to set this value in our docker-compose files


adding a MongoDB container:
i added a new service ( each container is a service), it the docker-compose.yml ( the shared one)
i didn't specified the version of the mongo image so it will download the latest one.
i don't do build: . ( because i don't make a new custom image, in mongo we just use the regular image - 
i don't need to customize it ) the 2 environments variables are written in the docs , that we have to set

mongo:
    image: mongo
    environment:
      - MONGO_INITDB_ROOT_USERNAME=...
      - MONGO_INITDB_ROOT_PASSWORD=...

when we destroys the db container all the db data goes as well - because we don't want to do than we 
will use voloumes.

    volumes:
      - ./:/app:ro 
      
      the bind mount which syncs the data within the container to a folder on my local drive 
      if i want to be able to poke around the database data on my local machine, then ill use 
      this bind mount , but for now ill just use the mongo client .
      
      - /app/node_modules

      i don't want to use an anonymous volume because i want to identify ( give a name to my db data )
      so ill create a named volume( which exactly like anonymous volume but it just have a human
      readable name  )

      /data/db - you can see in the docs of the mongo official image that this is the folder
      in the container that i'm interested in .

      volumes:
      -  /data/db - that the anonymous volume 

      volumes:
      - mongo-db:/data/db  - that a named volume

  don't be confused :
  in the bind mount - i provide a path on my local machine to a path on the container
  in the anonymous volume  - i just provide the path to the container directory( 
  that im interested in) 
  and for the name voloume - i do a name : provide the path to the container directory( 
  that im interested in) 
  but !!! its important , to note that with named volumes i also have to declare in another portion 
  in the docker-compose file and thats because a named volume can be used by multiple services.

  so im adding it in the section volumes( see in the docker-compose file)
  so now that we got persistance data for our database and the database container is up and running , lets set up our express application 
  to connect to our mongo database, when it comes to interacting with our mongo db we are going to use mongoose library.(which makes it a
  bit easier to talk to our mongo db) so ill install mongoose.



 i used docker inspect <container name > to get the ip address of our container and then put it in to our code , however 
 if we stop and restart my container or if i do docker-compose down and then up theres no guarantee that we get the same ip 
 address and even if we could guarantee the same exact ip address, the first time we run it we still need to get the ip address 
 manually from docker inspect. --> so docker has a nice feature that allows us to make it easy to talk between containers.
 and this feature only exist when it comes to custom networks that get created so if i do :
 docker network ls 
 well see theres a couple network that we have (we got the bridge and host network  - 2 default networks that comes with docker)
 and another one - <the name of our app/directory and ends with default> this is the one docker compose created 
( the custom one that was created just for our application)and when you have a custom networks not the 2 default ones - just networks that
i create ----> we have DNS , so when one docker container wants to talk to another docker container we can use the 
name of that container/service to talk to that container.

so if we go back to our docker-compose file - i see that my service for my node app called node-app, and the service for my mongo 
container called mongo, so i can refer to this containers ip address based of this service name. 
so i can put the service name instead of the ip 
example: instead of connect('mongodb://root:1234@172.30.0.2:27017/?authSource=admin')
write this - connect('mongodb://root:1234@mongo:27017/?authSource=admin')

to see how it works :
lets log  in to the node app container - docker exec -it perm-back-nodejs-node-app-1 bash - 
we can ping mongo - ping mongo